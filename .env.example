# =============================================================================
# AIDocumentIndexer - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control!
# =============================================================================

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
APP_NAME=AIDocumentIndexer
APP_ENV=development
# Options: development | staging | production

# Secret key for JWT tokens and encryption (min 32 characters)
SECRET_KEY=your-super-secret-key-change-this-in-production

# API URL (for frontend to connect to backend)
API_URL=http://localhost:8000

# Frontend URL
FRONTEND_URL=http://localhost:3000

# Debug mode (set to false in production)
DEBUG=true

# Development mode - enables dev token bypass for authentication
# WARNING: Never enable in production!
DEV_MODE=true

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# Database type: postgresql | sqlite | mysql
DATABASE_TYPE=postgresql

# PostgreSQL (recommended for production)
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/aidocindexer

# SQLite (for development/testing)
# DATABASE_URL=sqlite:///backend/data/aidocindexer.db

# MySQL (alternative)
# DATABASE_URL=mysql://user:password@localhost:3306/aidocindexer

# Connection pool settings
# Increased for better concurrency at scale:
# - 5 connections: ~15 concurrent users
# - 30 connections: ~100 concurrent users
# - 50+ connections: high-traffic production
DB_POOL_SIZE=30
DB_MAX_OVERFLOW=20

# =============================================================================
# VECTOR STORE CONFIGURATION
# =============================================================================
# Vector store backend: pgvector | qdrant | milvus | chroma | auto
# - pgvector: PostgreSQL + pgvector (recommended for < 1M documents)
# - qdrant: Qdrant vector database (recommended for 1-50M documents)
# - milvus: Milvus distributed (recommended for 50M+ documents)
# - chroma: ChromaDB local (development only)
# - auto: Auto-detect based on DATABASE_URL
VECTOR_STORE_BACKEND=auto

# ChromaDB settings (when using chroma backend)
CHROMA_PERSIST_DIRECTORY=./data/chroma
CHROMA_COLLECTION_NAME=documents

# Qdrant settings (when using qdrant backend)
# All open-source: Apache 2.0 license
QDRANT_URL=localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=documents

# Milvus settings (when using milvus backend)
# All open-source: Apache 2.0 license
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION=documents
MILVUS_USER=
MILVUS_PASSWORD=

# =============================================================================
# LLM PROVIDERS
# =============================================================================

# -----------------------------------------------------------------------------
# OpenAI (Primary cloud provider)
# -----------------------------------------------------------------------------
OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI organization (optional)
OPENAI_ORGANIZATION=

# OpenAI model settings
OPENAI_CHAT_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# -----------------------------------------------------------------------------
# Ollama (Local LLM)
# -----------------------------------------------------------------------------
# Enable Ollama integration
OLLAMA_ENABLED=true

# Ollama server URL
OLLAMA_HOST=http://localhost:11434

# Default Ollama model
OLLAMA_CHAT_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# -----------------------------------------------------------------------------
# Anthropic Claude (Optional)
# -----------------------------------------------------------------------------
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Default LLM Settings
# -----------------------------------------------------------------------------
# Which provider to use by default: openai | ollama | anthropic
DEFAULT_LLM_PROVIDER=openai

# Default models (can be overridden per task)
DEFAULT_CHAT_MODEL=gpt-4o
DEFAULT_EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimension (must match your embedding model)
EMBEDDING_DIMENSION=1536

# Temperature for chat responses (0.0 - 2.0)
DEFAULT_TEMPERATURE=0.7

# Max tokens for responses
DEFAULT_MAX_TOKENS=4096

# =============================================================================
# RAY CLUSTER CONFIGURATION
# =============================================================================
# Ray address: "auto" for local, or "ray://head-node:10001" for cluster
RAY_ADDRESS=auto

# Number of CPUs to use (leave empty for auto-detect)
RAY_NUM_CPUS=4

# Number of GPUs (if available)
RAY_NUM_GPUS=0

# Ray dashboard port
RAY_DASHBOARD_PORT=8265

# Object store memory (bytes, e.g., 2000000000 for 2GB)
RAY_OBJECT_STORE_MEMORY=

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
# Storage type: local | s3
STORAGE_TYPE=local

# Local storage path
STORAGE_PATH=/app/storage

# S3 Configuration (if STORAGE_TYPE=s3)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1
AWS_S3_BUCKET=aidocindexer-files

# Maximum file size for upload (in bytes, default 500MB)
MAX_UPLOAD_SIZE=524288000

# =============================================================================
# AUTHENTICATION
# =============================================================================
# NextAuth secret (generate with: openssl rand -base64 32)
NEXTAUTH_SECRET=your-nextauth-secret-here

# NextAuth URL (your frontend URL)
NEXTAUTH_URL=http://localhost:3000

# Session expiry (in seconds, default 24 hours)
SESSION_EXPIRY=86400

# Default admin credentials (change immediately after first login!)
ADMIN_EMAIL=admin@example.com
ADMIN_PASSWORD=changeme123

# =============================================================================
# DOCUMENT PROCESSING
# =============================================================================
# Processing mode: full | ocr | basic
# - full: Text + OCR + AI image analysis (most thorough, recommended)
# - ocr: Text + OCR for scanned documents
# - basic: Text extraction only (fastest)
DEFAULT_PROCESSING_MODE=full

# Chunk size for text splitting (tokens)
CHUNK_SIZE=512

# Chunk overlap (percentage)
CHUNK_OVERLAP=0.15

# OCR language (comma-separated for multiple)
OCR_LANGUAGES=en,de

# Enable image optimization
ENABLE_IMAGE_OPTIMIZATION=true

# Maximum image dimension (pixels) - larger images will be resized
MAX_IMAGE_DIMENSION=2048

# Skip images smaller than this (pixels) - likely decorative
MIN_IMAGE_DIMENSION=50

# =============================================================================
# RAG & SEARCH SETTINGS
# =============================================================================
# Query expansion (improves retrieval by 8-12%)
# Generates paraphrased versions of queries to improve recall
ENABLE_QUERY_EXPANSION=true

# Number of query variations to generate (1-5)
QUERY_EXPANSION_COUNT=2

# Document summarization for large files
# Reduces embedding tokens by 30-40% for documents over threshold
ENABLE_SUMMARIZATION=true

# Minimum document size (tokens) to trigger summarization
SUMMARIZATION_THRESHOLD=10000

# Self-RAG verification for answer quality
ENABLE_VERIFICATION=true

# Verification level: none | quick | standard | thorough
VERIFICATION_LEVEL=quick

# Similarity threshold for vector search (0.0-1.0)
# 0.55 is balanced for mixed content (OCR + clean text)
# Lower (0.4) for very noisy OCR, higher (0.7) for clean text only
SIMILARITY_THRESHOLD=0.55

# Default number of results to retrieve
DEFAULT_TOP_K=5

# =============================================================================
# ADVANCED RAG FEATURES (Million-Document Scale)
# =============================================================================
# These features improve retrieval quality and enable scaling to millions of docs.
# All use open-source components.

# -----------------------------------------------------------------------------
# Contextual Chunking (Anthropic's approach)
# -----------------------------------------------------------------------------
# Prepends LLM-generated context to each chunk before embedding
# Research shows 49-67% reduction in failed retrievals
CONTEXTUAL_CHUNKING_ENABLED=false
CONTEXT_GENERATION_PROVIDER=ollama
CONTEXT_GENERATION_MODEL=llama3.2

# -----------------------------------------------------------------------------
# Two-Stage Retrieval (Industry standard for scale)
# -----------------------------------------------------------------------------
# Stage 1: Fast ANN retrieval (150 candidates in <50ms)
# Stage 2: Precise reranking (ColBERT or cross-encoder)
# 15-30% better precision than single-stage
TWO_STAGE_RETRIEVAL_ENABLED=false
STAGE1_CANDIDATES=150

# -----------------------------------------------------------------------------
# ColBERT Reranking
# -----------------------------------------------------------------------------
# Late interaction model for more accurate relevance scoring
# 10-20% better precision than cross-encoders
# Open-source: colbert-ir/colbertv2.0 (MIT license)
COLBERT_RERANKING_ENABLED=false
COLBERT_MODEL=colbert-ir/colbertv2.0

# -----------------------------------------------------------------------------
# Embedding Cache
# -----------------------------------------------------------------------------
# In-memory cache size (100k = ~600MB for 1536-dim embeddings)
# For larger scale, use Redis-backed caching
EMBEDDING_CACHE_SIZE=100000

# Search result caching
SEARCH_CACHE_ENABLED=true
SEARCH_CACHE_TTL=300

# -----------------------------------------------------------------------------
# Embedding Quantization
# -----------------------------------------------------------------------------
# Quantization type: none | int8 | binary
# - none: Full float32 precision (default)
# - int8: 4x compression, 99%+ accuracy retention
# - binary: 32x compression, 92-96% accuracy retention
EMBEDDING_QUANTIZATION=none

# Matryoshka embedding dimensions
# Models that support Matryoshka (dimension reduction):
# - text-embedding-3-small/large (OpenAI)
# - nomic-embed-text-v1.5 (open-source)
# - jina-embeddings-v3 (open-source)
# Set to smaller value (256, 512) for faster search, or leave unset for full dimensions
# MATRYOSHKA_DIMENSIONS=1536

# =============================================================================
# ASYNC PROCESSING (Celery)
# =============================================================================
# Enable Celery for async document processing
# Required for handling bulk uploads without blocking API
CELERY_ENABLED=false
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
CELERY_WORKER_CONCURRENCY=4

# =============================================================================
# OPTIONAL FEATURES
# =============================================================================

# -----------------------------------------------------------------------------
# File Watcher
# -----------------------------------------------------------------------------
# Enable automatic file watching
ENABLE_FILE_WATCHER=false

# Directories to watch (comma-separated)
WATCH_DIRECTORIES=/path/to/watch1,/path/to/watch2

# Default access tier for auto-indexed files
WATCH_DEFAULT_TIER=30

# -----------------------------------------------------------------------------
# Web Scraping
# -----------------------------------------------------------------------------
# Enable web scraping feature
ENABLE_WEB_SCRAPING=true

# Maximum pages to scrape per request
MAX_SCRAPE_PAGES=10

# Scraping timeout (seconds)
SCRAPE_TIMEOUT=30

# -----------------------------------------------------------------------------
# Multi-LLM Collaboration
# -----------------------------------------------------------------------------
# Enable multi-LLM collaboration mode
ENABLE_MULTI_LLM=true

# Minimum tier level to use multi-LLM feature
MULTI_LLM_MIN_TIER=50

# =============================================================================
# REDIS (Caching & Sessions)
# =============================================================================
REDIS_URL=redis://localhost:6379/0

# Cache TTL (seconds)
CACHE_TTL=3600

# =============================================================================
# LOGGING
# =============================================================================
# Log level: DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_LEVEL=INFO

# Log format: json | text
LOG_FORMAT=text

# Log file path (leave empty for stdout only)
LOG_FILE=

# =============================================================================
# RATE LIMITING
# =============================================================================
# Requests per minute per user
RATE_LIMIT_REQUESTS=60

# Rate limit window (seconds)
RATE_LIMIT_WINDOW=60

# =============================================================================
# CORS (Cross-Origin Resource Sharing)
# =============================================================================
# Allowed origins (comma-separated, or * for all)
CORS_ORIGINS=http://localhost:3000

# =============================================================================
# INTERNATIONALIZATION
# =============================================================================
# Default locale
DEFAULT_LOCALE=en

# Supported locales (comma-separated)
SUPPORTED_LOCALES=en,de

# =============================================================================
# DOCUMENT GENERATION SETTINGS
# =============================================================================
# Include images in generated documents (PPTX, DOCX)
GENERATION_INCLUDE_IMAGES=false

# Image backend for document generation
# - unsplash: Free stock photos (requires UNSPLASH_ACCESS_KEY)
# - stability: Stability AI API for AI-generated images (requires STABILITY_API_KEY)
# - automatic1111: Local Stable Diffusion via Automatic1111 API (requires running server)
GENERATION_IMAGE_BACKEND=unsplash

# Unsplash API key (https://unsplash.com/developers)
UNSPLASH_ACCESS_KEY=

# Stability AI API key (https://platform.stability.ai/)
STABILITY_API_KEY=

# Automatic1111 API URL (for local Stable Diffusion)
AUTOMATIC1111_URL=http://localhost:7860

# =============================================================================
# FEATURE FLAGS
# =============================================================================
# Enable document generation
FEATURE_DOC_GENERATION=true

# Enable chat export
FEATURE_CHAT_EXPORT=true

# Enable analytics dashboard
FEATURE_ANALYTICS=true

# =============================================================================
# MONITORING & OBSERVABILITY
# =============================================================================
# Sentry DSN for error tracking (leave empty to disable)
# Get your DSN from https://sentry.io
SENTRY_DSN=

# Sentry traces sample rate (0.0 to 1.0)
# 0.1 = 10% of requests are traced for performance monitoring
SENTRY_TRACES_SAMPLE_RATE=0.1

# Sentry profiles sample rate (0.0 to 1.0)
# 0.1 = 10% of traced requests are profiled
SENTRY_PROFILES_SAMPLE_RATE=0.1

# Enable Prometheus metrics endpoint at /metrics
ENABLE_METRICS=true

# =============================================================================
# DATABASE CONNECTOR (Natural Language Database Querying)
# =============================================================================
# Maximum rows returned from database queries (safety limit)
DATABASE_CONNECTOR_MAX_ROWS=1000

# Query timeout in seconds
DATABASE_CONNECTOR_TIMEOUT=30

# Enable query caching for repeated natural language queries
DATABASE_CONNECTOR_CACHE_ENABLED=true
